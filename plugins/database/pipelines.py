"""
æ•°æ®åº“å¯¼å‡ºç®¡é“
"""
import logging
from typing import Dict, List, Any
from datetime import datetime

from BlockchainSpider.pipelines.sync import unpacked_sync_item
from .config import DatabaseConfig
from .adapter import PostgreSQLAdapter
from .models import MODEL_MAPPING, FIELD_MAPPING
from .utils import convert_item_to_dict, handle_jsonb_fields, clean_data


class DatabasePipeline:
    """
    æ•°æ®åº“å¯¼å‡ºç®¡é“
    
    æ”¯æŒå°† BlockchainSpider çˆ¬å–çš„æ•°æ®ç›´æŽ¥å†™å…¥ PostgreSQL æ•°æ®åº“
    """
    
    def __init__(self):
        self.config = None
        self.adapter = None
        self.batch_data = {}
        self.batch_size = 1000
        self.logger = logging.getLogger(self.__class__.__name__)
        self.used_models = set()  # è®°å½•å®žé™…ä½¿ç”¨çš„æ¨¡åž‹
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_items': 0,
            'inserted_items': 0,
            'failed_items': 0,
            'duplicate_items': 0,
        }
    
    def open_spider(self, spider):
        """åˆå§‹åŒ–æ•°æ®åº“è¿žæŽ¥"""
        try:
            self.logger.info('')
            self.logger.info('â•”' + 'â•' * 58 + 'â•—')
            self.logger.info('â•‘' + ' ' * 10 + 'ðŸš€ Database Pipeline Starting' + ' ' * 18 + 'â•‘')
            self.logger.info('â•š' + 'â•' * 58 + 'â•')
            self.logger.info('')
            
            # ä»Žçˆ¬è™«èŽ·å–é…ç½®
            self.logger.info('ðŸ“ Loading database configuration...')
            self.config = DatabaseConfig.from_spider(spider)
            self.batch_size = self.config.batch_size
            self.logger.info(f'âœ“ Batch size: {self.batch_size}')
            self.logger.info(f'âœ“ Auto create tables: {self.config.auto_create_tables}')
            self.logger.info(f'âœ“ Database: {self.config.database}')
            self.logger.info('')
            
            # åˆ›å»ºæ•°æ®åº“é€‚é…å™¨
            self.logger.info('ðŸ”§ Initializing PostgreSQL adapter...')
            self.adapter = PostgreSQLAdapter(self.config)
            self.adapter.connect()
            
            # æ³¨æ„ï¼šæš‚ä¸åˆ›å»ºè¡¨ï¼Œç­‰æ”¶åˆ°ç¬¬ä¸€ä¸ªæ•°æ®é¡¹æ—¶æŒ‰éœ€åˆ›å»º
            # è¿™æ ·å¯ä»¥é¿å…åˆ›å»ºä¸éœ€è¦çš„è¡¨
            
            # å¥åº·æ£€æŸ¥
            self.logger.info('ðŸ¥ Performing health check...')
            if not self.adapter.health_check():
                raise RuntimeError('âŒ Database health check failed')
            self.logger.info('âœ… Health check passed!')
            
            self.logger.info('')
            self.logger.info('â•”' + 'â•' * 58 + 'â•—')
            self.logger.info('â•‘' + ' ' * 8 + 'âœ… Database Pipeline Ready!' + ' ' * 20 + 'â•‘')
            self.logger.info('â•‘' + f' Type: {self.config.db_type}'.ljust(59) + 'â•‘')
            self.logger.info('â•‘' + f' Host: {self.config.host}:{self.config.port}'.ljust(59) + 'â•‘')
            self.logger.info('â•‘' + f' Database: {self.config.database}'.ljust(59) + 'â•‘')
            self.logger.info('â•š' + 'â•' * 58 + 'â•')
            self.logger.info('')
            
        except Exception as e:
            self.logger.error(f'Failed to initialize database pipeline: {e}')
            raise
    
    @unpacked_sync_item
    def process_item(self, item, spider):
        """å¤„ç†æ•°æ®é¡¹"""
        if not self.adapter:
            return item
        
        try:
            self.stats['total_items'] += 1
            
            # èŽ·å–å¯¹åº”çš„æ¨¡åž‹ç±»
            item_class_name = item.__class__.__name__
            model_class = MODEL_MAPPING.get(item_class_name)
            
            if not model_class:
                self.logger.debug(f'No model mapping for {item_class_name}, skipping')
                return item
            
            # æŒ‰éœ€åˆ›å»ºè¡¨ï¼ˆç¬¬ä¸€æ¬¡é‡åˆ°è¿™ç§ç±»åž‹æ—¶ï¼‰
            if model_class not in self.used_models:
                self._ensure_table_exists(model_class)
                self.used_models.add(model_class)
            
            # è½¬æ¢æ•°æ®æ ¼å¼
            data = self._convert_and_clean_item(item, item_class_name)
            
            # æ·»åŠ åˆ°æ‰¹é‡æ•°æ®
            if item_class_name not in self.batch_data:
                self.batch_data[item_class_name] = []
            
            self.batch_data[item_class_name].append(data)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰¹é‡æ’å…¥
            if len(self.batch_data[item_class_name]) >= self.batch_size:
                self._flush_batch(item_class_name, model_class)
            
            return item
            
        except Exception as e:
            self.logger.error(f'Error processing item: {e}', exc_info=True)
            self.stats['failed_items'] += 1
            return item
    
    def _ensure_table_exists(self, model_class):
        """
        ç¡®ä¿è¡¨å­˜åœ¨ï¼ˆæŒ‰éœ€åˆ›å»ºï¼‰
        
        Args:
            model_class: æ•°æ®åº“æ¨¡åž‹ç±»
        """
        if not self.config.auto_create_tables:
            return
        
        try:
            self.adapter.create_tables(models_to_create=[model_class])
            self.logger.info(f'ðŸ“Š Created table: {model_class.__tablename__}')
        except Exception as e:
            self.logger.warning(f'Failed to create table {model_class.__tablename__}: {e}')
    
    def _convert_and_clean_item(self, item, item_class_name: str) -> Dict[str, Any]:
        """
        è½¬æ¢å’Œæ¸…ç†æ•°æ®é¡¹
        
        Args:
            item: Scrapy Item å¯¹è±¡
            item_class_name: Item ç±»å
            
        Returns:
            æ¸…ç†åŽçš„æ•°æ®å­—å…¸
        """
        # è½¬æ¢ä¸ºå­—å…¸
        data = convert_item_to_dict(item, FIELD_MAPPING)
        
        # æ¸…ç†æ•°æ®
        data = clean_data(data)
        
        # å¤„ç†ç‰¹æ®Šå­—æ®µ
        data = self._handle_special_fields(data, item_class_name)
        
        # æ·»åŠ åˆ›å»ºæ—¶é—´
        if 'created_at' not in data:
            data['created_at'] = datetime.utcnow()
        
        return data
    
    def _handle_special_fields(self, data: Dict[str, Any], item_class_name: str) -> Dict[str, Any]:
        """
        å¤„ç†ç‰¹æ®Šå­—æ®µ
        
        Args:
            data: æ•°æ®å­—å…¸
            item_class_name: Item ç±»å
            
        Returns:
            å¤„ç†åŽçš„æ•°æ®å­—å…¸
        """
        # EventLogItem: å¤„ç† topics å­—æ®µ
        if item_class_name == 'EventLogItem':
            # topics å­—æ®µéœ€è¦è½¬æ¢ä¸º JSONB
            if 'topics' in data and not isinstance(data['topics'], (list, dict)):
                data['topics'] = []
            data = handle_jsonb_fields(data, ['topics'])
        
        # Token1155TransferItem: å¤„ç†æ•°ç»„å­—æ®µ
        if item_class_name == 'Token1155TransferItem':
            data = handle_jsonb_fields(data, ['token_ids', 'values'])
        
        # TraceItem: å¤„ç† trace_address å­—æ®µ
        if item_class_name == 'TraceItem':
            data = handle_jsonb_fields(data, ['trace_address'])
        
        # LabelReportItem: å¤„ç†æ‰€æœ‰ JSON å­—æ®µ
        if item_class_name == 'LabelReportItem':
            data = handle_jsonb_fields(data, ['labels', 'urls', 'addresses', 'transactions'])
        
        # TronTransactionItem: å¤„ç† raw_data å­—æ®µ
        if item_class_name == 'TronTransactionItem':
            data = handle_jsonb_fields(data, ['raw_data'])
        
        # ABIItem: å¤„ç† abi å­—æ®µ
        if item_class_name == 'ABIItem':
            data = handle_jsonb_fields(data, ['abi'])
        
        # SolanaInstructionItem åŠå…¶å­ç±»: å¤„ç† JSON å­—æ®µ
        if item_class_name in ['SolanaInstructionItem', 'SPLTokenActionItem', 'SPLMemoItem', 'ValidateVotingItem', 'SystemItem']:
            data = handle_jsonb_fields(data, ['accounts', 'info'])
        
        return data
    
    def _flush_batch(self, item_class_name: str, model_class):
        """
        æ‰§è¡Œæ‰¹é‡æ’å…¥
        
        Args:
            item_class_name: Item ç±»å
            model_class: æ•°æ®åº“æ¨¡åž‹ç±»
        """
        if item_class_name not in self.batch_data:
            return
        
        data_list = self.batch_data[item_class_name]
        if not data_list:
            return
        
        try:
            inserted_count = self.adapter.insert_batch(model_class, data_list)
            
            self.stats['inserted_items'] += inserted_count
            self.stats['duplicate_items'] += len(data_list) - inserted_count
            
            # ä½¿ç”¨æ›´é†’ç›®çš„æ—¥å¿—
            if inserted_count == len(data_list):
                self.logger.info(
                    f'ðŸ’¾ Inserted {inserted_count} {item_class_name} â†’ {model_class.__tablename__}'
                )
            else:
                self.logger.info(
                    f'ðŸ’¾ Inserted {inserted_count}/{len(data_list)} {item_class_name} â†’ '
                    f'{model_class.__tablename__} (skipped {len(data_list) - inserted_count} duplicates)'
                )
            
        except Exception as e:
            self.logger.error(
                f'Failed to flush batch for {item_class_name}: {e}',
                exc_info=True
            )
            self.stats['failed_items'] += len(data_list)
            
        finally:
            self.batch_data[item_class_name] = []
    
    def close_spider(self, spider):
        """å…³é—­çˆ¬è™«æ—¶å¤„ç†å‰©ä½™æ•°æ®"""
        try:
            # å¤„ç†å‰©ä½™çš„æ‰¹é‡æ•°æ®
            for item_class_name, data_list in self.batch_data.items():
                if data_list:
                    model_class = MODEL_MAPPING.get(item_class_name)
                    if model_class:
                        inserted_count = self.adapter.insert_batch(model_class, data_list)
                        
                        self.stats['inserted_items'] += inserted_count
                        self.stats['duplicate_items'] += len(data_list) - inserted_count
                        
                        self.logger.info(
                            f'Final flush: {inserted_count}/{len(data_list)} '
                            f'{item_class_name} records'
                        )
            
            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            self._print_stats()
            
            # å…³é—­æ•°æ®åº“è¿žæŽ¥
            if self.adapter:
                self.adapter.disconnect()
            
            self.logger.info('Database pipeline closed')
            
        except Exception as e:
            self.logger.error(f'Error closing database pipeline: {e}', exc_info=True)
    
    def _print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        self.logger.info('')
        self.logger.info('â•”' + 'â•' * 58 + 'â•—')
        self.logger.info('â•‘' + ' ' * 15 + 'ðŸ“Š Final Statistics' + ' ' * 23 + 'â•‘')
        self.logger.info('â• ' + 'â•' * 58 + 'â•£')
        self.logger.info('â•‘' + f'  Total items processed: {self.stats["total_items"]}'.ljust(59) + 'â•‘')
        self.logger.info('â•‘' + f'  âœ… Successfully inserted: {self.stats["inserted_items"]}'.ljust(59) + 'â•‘')
        self.logger.info('â•‘' + f'  â­ï¸  Duplicate items: {self.stats["duplicate_items"]}'.ljust(59) + 'â•‘')
        self.logger.info('â•‘' + f'  âŒ Failed items: {self.stats["failed_items"]}'.ljust(59) + 'â•‘')
        self.logger.info('â•š' + 'â•' * 58 + 'â•')
        self.logger.info('')
        
        if self.stats['inserted_items'] > 0:
            self.logger.info('ðŸŽ‰ Data successfully saved to PostgreSQL database!')
            self.logger.info(f'   Database: {self.config.database}')
            self.logger.info(f'   Location: {self.config.host}:{self.config.port}')
            self.logger.info('') 